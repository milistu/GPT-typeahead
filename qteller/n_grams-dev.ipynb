{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9509b88bb26749ffb24b939c988bf30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9d120acca1465aa91e6fb02add99b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734516dbb7e04cdc9edc8c8950045b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/295k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eccd78c017444c9e887f60b392359c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/66.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9849734826aa4e0494cd6f337699e9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/64.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5153850b66444679a39e40c77bda7d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f206133d05004dc68d2166129f8ce08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f84c7a98cd243499c6a5958c59bfb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_id = \"google_wellformed_query\"\n",
    "dataset = load_dataset(path=dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The European Union includes how many ?',\n",
       " 'What are Mia Hamms accomplishment ?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"content\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Language Modeling\n",
    "- 🎯 **Goal** - compute the probability of a sentence or sequence of words in our case:\n",
    "$$ P(W) = (W_1, W_2, W_3, W_4, W_5 ... W_n) $$ \n",
    "- 🖇️ **Related task** - probability of an upcoming word:\n",
    "$$ P(W_5|W_1, W_2, W_3, W_4) $$\n",
    "\n",
    "Basically these model is telling us how well this words fit together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to compute P(W)\n",
    "- How to compute this joint probability:\n",
    "$$ P(its, water, is, so, transparent, that) $$\n",
    "- Intuition: let's rely on the `Chain Rule of Probability`\n",
    "\n",
    "#### Reminder: The Chain Rule\n",
    "- Recall the definition of conditional probabilities\n",
    "$$ P(A|B) = {P(A,B) \\over P(B)} $$\n",
    "$$ P(A|B)P(B) = P(A,B) $$\n",
    "$$ P(A,B) = P(A|B)P(B)$$\n",
    "- The Chain Rule in General\n",
    "$$ P(x_1, x_2, x_3, ... , x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1, x_2) ... P(x_n|x_1, ... ,x_{n-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chain Rule applied to compute joint probability of words in sentence\n",
    "$$ P(\\text{\"its water is so transparent that\"}) = $$\n",
    "$$ P(its)P(water| its)P(is|its water)P(so|its water is)P(transparent|its water is so)P(that|its water is so transparent) $$\n",
    "\n",
    "\n",
    "$$ P(w_1, w_2 ... w_n) = \\prod_i P(w_i|w_1, w_2 ... w_{i-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Assumption by Andrei Markov\n",
    "- Simplifying assumption:\n",
    "$$ P(the|\\text{its water is so transparent that}) \\approx P(the|that) $$\n",
    "- Or maybe:\n",
    "$$ P(the|\\text{its water is so transparent that}) \\approx P(the|\\text{that transparent}) $$\n",
    "- More formally:\n",
    "$$ P(w_1, w_2 ... w_n) \\approx \\prod_i P(w_i|w_{i-k} ... w_{i-1}) $$\n",
    "The probability of sequence of words is the product of conditional probability of that word given some prefix of last few words.\n",
    "In other words, we approximate each component in the product\n",
    "$$ P(w_i|w_1w_2 ... w_{i-1}) \\approx P(w_i|w_{i-k} ... w_{i-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplest case: Unigram model\n",
    "$$ P(w_1, w_2 ... w_n) \\approx \\prod_i P(w_i) $$\n",
    "\n",
    "Some automatically generated sentences from a unigram model:\n",
    "\n",
    "thrift, did, eighty, said, hard, 'm ...\n",
    "\n",
    "We would get random sequence of words that woud not look anything like sentences.\n",
    "\n",
    "#### Bigram model\n",
    "- Condition on the previous word:\n",
    "$$ P(w_i|w_1w_2 ... w_{i-1}) \\approx P(w_i|w_{i-1}) $$\n",
    "\n",
    "Some automatically generated sentences from a unigram model:\n",
    "\n",
    "outside, new, car, parking, lot, of, the, agreement ...\n",
    "\n",
    "### N-gram models\n",
    "\n",
    "- We can extend to trigrams, 4-grams, 5-grams\n",
    "- In general this is an insufficient model of language\n",
    "    - because language has <span style=\"color:green;\">long-distance dependencies</span>:<br>\n",
    "    \"The computer which I had just put into the machine room on the fifth floor crashed.\"\n",
    "- But we can often get away with N-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
