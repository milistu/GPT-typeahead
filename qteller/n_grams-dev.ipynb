{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"google_wellformed_query\"\n",
    "dataset = load_dataset(path=dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The European Union includes how many ?',\n",
       " 'What are Mia Hamms accomplishment ?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"content\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model - Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Language Modeling\n",
    "---\n",
    "- üéØ **Goal** - compute the probability of a sentence or sequence of words in our case:\n",
    "$$ P(W) = (W_1, W_2, W_3, W_4, W_5 ... W_n) $$ \n",
    "- üñáÔ∏è **Related task** - probability of an upcoming word:\n",
    "$$ P(W_5|W_1, W_2, W_3, W_4) $$\n",
    "\n",
    "Basically these model is telling us how well this words fit together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to compute P(W)\n",
    "---\n",
    "- How to compute this joint probability:\n",
    "$$ P(its, water, is, so, transparent, that) $$\n",
    "- Intuition: let's rely on the `Chain Rule of Probability`\n",
    "\n",
    "#### Reminder: The Chain Rule\n",
    "---\n",
    "- Recall the definition of conditional probabilities\n",
    "$$ P(A|B) = {P(A,B) \\over P(B)} $$\n",
    "$$ P(A|B)P(B) = P(A,B) $$\n",
    "$$ P(A,B) = P(A|B)P(B)$$\n",
    "- The Chain Rule in General\n",
    "$$ P(x_1, x_2, x_3, ... , x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1, x_2) ... P(x_n|x_1, ... ,x_{n-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chain Rule applied to compute joint probability of words in sentence\n",
    "---\n",
    "$$ P(\\text{\"its water is so transparent that\"}) = $$\n",
    "$$ P(its)P(water| its)P(is|its water)P(so|its water is)P(transparent|its water is so)P(that|its water is so transparent) $$\n",
    "\n",
    "\n",
    "$$ P(w_1, w_2 ... w_n) = \\prod_i P(w_i|w_1, w_2 ... w_{i-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Assumption by Andrei Markov\n",
    "---\n",
    "- Simplifying assumption:\n",
    "$$ P(the|\\text{its water is so transparent that}) \\approx P(the|that) $$\n",
    "- Or maybe:\n",
    "$$ P(the|\\text{its water is so transparent that}) \\approx P(the|\\text{that transparent}) $$\n",
    "- More formally:\n",
    "$$ P(w_1, w_2 ... w_n) \\approx \\prod_i P(w_i|w_{i-k} ... w_{i-1}) $$\n",
    "The probability of sequence of words is the product of conditional probability of that word given some prefix of last few words.\n",
    "In other words, we approximate each component in the product\n",
    "$$ P(w_i|w_1w_2 ... w_{i-1}) \\approx P(w_i|w_{i-k} ... w_{i-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplest case: Unigram model\n",
    "---\n",
    "$$ P(w_1, w_2 ... w_n) \\approx \\prod_i P(w_i) $$\n",
    "\n",
    "Some automatically generated sentences from a unigram model:\n",
    "\n",
    "thrift, did, eighty, said, hard, 'm ...\n",
    "\n",
    "We would get random sequence of words that woud not look anything like sentences.\n",
    "\n",
    "#### Bigram model\n",
    "---\n",
    "- Condition on the previous word:\n",
    "$$ P(w_i|w_1w_2 ... w_{i-1}) \\approx P(w_i|w_{i-1}) $$\n",
    "\n",
    "Some automatically generated sentences from a unigram model:\n",
    "\n",
    "outside, new, car, parking, lot, of, the, agreement ...\n",
    "\n",
    "### N-gram models\n",
    "---\n",
    "\n",
    "- We can extend to trigrams, 4-grams, 5-grams\n",
    "- In general this is an insufficient model of language\n",
    "    - because language has <span style=\"color:green;\">long-distance dependencies</span>:<br>\n",
    "    \"The computer which I had just put into the machine room on the fifth floor crashed.\"\n",
    "- But we can often get away with N-gram models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calaculate probabilities?\n",
    "### Unigram Probability\n",
    "---\n",
    "$$ \\text{Context Word Count} \\over \\text{Vocab Count} $$\n",
    "\n",
    "How many times does the word occur in your corpus derived with number of words in whole corpus.\n",
    "\n",
    "### Bi-gram Probability\n",
    "---\n",
    "$$ P(y | x) = {Count(x, y) \\over Count(x)} $$\n",
    "$$ P(\\text{Next\\_Word} | \\text{Current\\_Word}) = \\frac{Count(\\text{Current\\_Word}, \\text{Next\\_Word})} {Count(\\text{Current\\_Word})} $$\n",
    "\n",
    "Probabilitty of next word **y** given previous word **x** - How many times both word (**x** and **y**) occur together in corpus derived with number of times current word **x** occur in whole corpus. \n",
    "\n",
    "### N-gram Probability\n",
    "---\n",
    "$$ P(w_n|w_1^{N-1}) = \\frac{Count(w_1^{N-1}, w_n)}{Count(w_1^{N-1})} $$\n",
    "$$ P(\\text{A word from vocab}|\\text{Previous ngram Tuples}) = \\frac{Count(\\text{Previous ngram Tuples}, \\text{A word from vocab})}{Count(\\text{Previous ngram Tuples})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_n_grams(tokenized_sentences, ngram):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_n_grams(self, tokenized_sentences, ngram):\n",
    "    \"\"\"\n",
    "    Creates n-gram from tokenized sentence and counts the same\n",
    "    \"\"\"\n",
    "    freq = defaultdict(lambda: 0)\n",
    "    for sentence in tqdm(tokenized_sentences, desc=\"NGrams\"):\n",
    "        sentence = [self._start_token] * ngram + sentence + [self._end_token]\n",
    "        m = len(sentence) if ngram == 1 else len(sentence) - 1\n",
    "        for i in range(m):\n",
    "            ngram_token = sentence[i : i + ngram]\n",
    "            # freq[tuple(ngram_token)] += 1\n",
    "            # tuples can't be used as key in JSON\n",
    "            freq[\" \".join(ngram_token)] += 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
